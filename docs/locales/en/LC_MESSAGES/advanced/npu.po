# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2024, LlamaFactory team.
# This file is distributed under the same license as the LLaMA Factory
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: LLaMA Factory \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-03-05 01:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: en\n"
"Language-Team: en <LL@li.org>\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.16.0\n"

#: ../../source/advanced/npu.rst:2 0183134a35df482bbb9eea1da7203468
msgid "华为 NPU 适配"
msgstr "NPU"

#: ../../source/advanced/npu.rst:4 d1842cf3d10545fdb8e3544c7dbbfe0e
msgid ""
"目前LLaMA-Factory 通过 torch-npu 库完成了对华为昇腾 910b 系列芯片的支持, 包含 32GB 和 64GB "
"两个版本。跟其他使用相比，会需要额外3个前置条件"
msgstr ""

#: ../../source/advanced/npu.rst:6 9fe74255154c4b979f8858ad85f92eb9
msgid "加速卡本身的驱动正常安装"
msgstr ""

#: ../../source/advanced/npu.rst:7 ca849dbc982e4d7e97332beac3255766
msgid "CANN Toolkit 和 Kernels库正常安装"
msgstr ""

#: ../../source/advanced/npu.rst:8 36d342e0e79a409aa94881f0746fdd6c
msgid "torch-npu 库正常安装"
msgstr ""

#: ../../source/advanced/npu.rst:10 1b1874df193f48509538942717e8d906
msgid ""
"为方便昇腾用户使用，LLaMA-Factory 提供已预装昇腾环境的 :ref:`install_form_docker` "
"及自行安装昇腾环境，:ref:`install_form_pip` 两种方式，可按需自行选择："
msgstr ""

#: ../../source/advanced/npu.rst:15 4a14d717cdd242588dbbbcfcd8dff5b3
msgid "Docker 安装"
msgstr "Install By Docker"

#: ../../source/advanced/npu.rst:18 b3c0d782fc294db1865bf81516acceb6
msgid ""
"请确保宿主机已根据昇腾卡型号成功安装对应的固件和驱动，可参考 `快速安装昇腾环境 "
"<https://ascend.github.io/docs/sources/ascend/quick_install.html>`_ 指引。"
msgstr ""

#: ../../source/advanced/npu.rst:20 82e5cdb6660d47799f545f85498c86d6
msgid ""
"LLaMA-Factory 提供 :ref:`docker_compose` 和 :ref:`docker_build` "
"两种构建方式，请根据需求选择其一。"
msgstr ""

#: ../../source/advanced/npu.rst:26 1e81eece9dd54b64a7bf73bdd5568d7e
msgid "使用 docker-compose 构建并启动 docker 容器"
msgstr ""

#: ../../source/advanced/npu.rst:28 021f56fb2041400d96644baefd48fc1a
msgid "进入 LLaMA-Factory 项目中存放 Dockerfile 及 docker-compose.yaml 的 docker-npu 目录："
msgstr ""

#: ../../source/advanced/npu.rst:35 fb78dc91e7f24849a548292b31f677c3
msgid "构建 docker 镜像并启动 docker 容器："
msgstr ""

#: ../../source/advanced/npu.rst:41 ../../source/advanced/npu.rst:85
#: 7826389aa4934300a8489438d9b9da4b fab99eaec11049bb9d424da6f86e2110
msgid "进入 docker 容器："
msgstr ""

#: ../../source/advanced/npu.rst:52 16941789662a408d8b321edd3cb6fb23
msgid "不使用 docker-compose"
msgstr ""

#: ../../source/advanced/npu.rst:54 95c3a49001604c9a8a254c72ef046f30
msgid "使用 docker build 直接构建 docker 镜像："
msgstr ""

#: ../../source/advanced/npu.rst:61 0fd48d087e3e43f4ab51232d4b029241
msgid "启动 docker 容器："
msgstr ""

#: ../../source/advanced/npu.rst:95 2951a3b18a3f47b5ad86feaa0be1c554
msgid "自行 pip 安装"
msgstr "Install By pip"

#: ../../source/advanced/npu.rst:97 f8a87444aec6491bbeba58b585065078
msgid "自行 pip 安装时， python 版本建议使用3.10， 目前该版本对于 NPU 的使用情况会相对稳定，其他版本可能会遇到一些未知的情况"
msgstr ""

#: ../../source/advanced/npu.rst:100 103c9d4f75df4c7882fb767e31a1dac3
msgid "依赖1: NPU 驱动"
msgstr ""

#: ../../source/advanced/npu.rst:102 2ea6966103e14a87ab91b04075940af1
msgid ""
"根据昇腾卡型号安装对应的固件和驱动，可参考 `快速安装昇腾环境 "
"<https://ascend.github.io/docs/sources/ascend/quick_install.html>`_ 指引，使用"
" ``npu-smi info`` 验证如下"
msgstr "Verification"

#: ../../source/advanced/npu.rst:107 7f67146b75f943faacb3416a964e3327
msgid "依赖2: NPU 开发包"
msgstr ""

#: ../../source/advanced/npu.rst:109 fd2e3680f1de42498bec76d1ebd318b2
msgid "相关包建议版本"
msgstr ""

#: ../../source/advanced/npu.rst:113 ad5285d028d74c20a56feed7e7b3715e
msgid "Requirement"
msgstr ""

#: ../../source/advanced/npu.rst:114 8dd6c7a348fa4331b97284302879d4c0
msgid "Minimum"
msgstr ""

#: ../../source/advanced/npu.rst:115 1ba57d1e5d4e4776b28ca045212fc8bd
msgid "Recommend"
msgstr ""

#: ../../source/advanced/npu.rst:116 e14c391f3b74414486a8501865efb538
msgid "CANN"
msgstr ""

#: ../../source/advanced/npu.rst:117 ../../source/advanced/npu.rst:118
#: 8d7f601b0ad64b3b941a75524820f922 ff77786776ef4621a5c5223904da9ae7
msgid "8.0.RC1"
msgstr ""

#: ../../source/advanced/npu.rst:119 cf3a0ec6d70a43279803902f6f2e323f
msgid "torch"
msgstr ""

#: ../../source/advanced/npu.rst:120 ../../source/advanced/npu.rst:121
#: ../../source/advanced/npu.rst:123 8b5c2f3547ae48ef8d2f3b3e398861fc
#: a8081820f7564d7cad1842289b19236a f2b9d5f74e7f4522a450e4415e92b7e6
msgid "2.1.0"
msgstr ""

#: ../../source/advanced/npu.rst:122 45f295a143a04199b81c7817598f57ee
msgid "torch-npu"
msgstr ""

#: ../../source/advanced/npu.rst:124 d4bf1b3f08794f439823f1b1cba5008b
msgid "2.1.0.post3"
msgstr ""

#: ../../source/advanced/npu.rst:125 932f31c1ad714f4bb8dfdc7862f9d0a1
msgid "deepspeed"
msgstr ""

#: ../../source/advanced/npu.rst:126 ../../source/advanced/npu.rst:127
#: 37bdc96b13ed4f138bbf9e760742c028 e53d03ae0b3c4366b760ff375f103026
msgid "0.13.2"
msgstr ""

#: ../../source/advanced/npu.rst:129 b07a8039a00842639e5b4f7f1e3b0e82
msgid ""
"可以按照 `快速安装昇腾环境 "
"<https://ascend.github.io/docs/sources/ascend/quick_install.html>`_ "
"指引，或者使用以下命令完成快速安装："
msgstr ""

#: ../../source/advanced/npu.rst:149 ec15728564cd48eb8472f9cc12167acc
msgid "依赖3: torch-npu"
msgstr ""

#: ../../source/advanced/npu.rst:151 203709c863f34bca90335fade9da65d4
msgid "依赖3建议在安装 LLaMA-Factory 的时候一起选配安装， 把 ``torch-npu`` 一起加入安装目标，命令如下"
msgstr ""

#: ../../source/advanced/npu.rst:158 5b47115280b7440e9087336e83a33976
msgid "依赖校验"
msgstr ""

#: ../../source/advanced/npu.rst:159 fc7105965c9f43b7a1bf4ce6c7f2ea53
msgid "3个依赖都安装后，可以通过如下的 python 脚本对 ``torch_npu`` 的可用情况做一下校验"
msgstr ""

#: ../../source/advanced/npu.rst:167 3b2599174474486ea608807ac8b795c9
msgid "预期结果是打印true"
msgstr ""

#: ../../source/advanced/npu.rst:172 652eb434d1f64791bb92ccf15481ed82
msgid "安装校验"
msgstr "Verification"

#: ../../source/advanced/npu.rst:174 f2b25c786d2a4fb8aa9e2ea1818620a5
msgid "使用以下指令对 LLaMA-Factory × 昇腾的安装进行校验："
msgstr ""

#: ../../source/advanced/npu.rst:180 d6d0219165ee446c8380f40a2723471a
msgid "如下所示，正确显示 LLaMA-Factory、PyTorch NPU 和 CANN 版本号及 NPU 型号等信息即说明安装成功。"
msgstr ""

#: ../../source/advanced/npu.rst:198 25d00115598c41459d198289e1584be6
msgid "在 LLaMA-Factory 中使用 NPU"
msgstr "Using NPU in LLaMA-Factory"

#: ../../source/advanced/npu.rst:199 cfcac1bf3e3842ffb7fe76266072c767
msgid ""
"前面依赖安装完毕和完成校验后，即可像文档的其他部分一样正常使用 ``llamafactory-cli`` 的相关功能， NPU "
"的使用是无侵入的。主要的区别是需要修改一下命令行中 设备变量使用 将原来的 Nvidia 卡的变量 "
"``CUDA_VISIBLE_DEVICES`` 替换为 ``ASCEND_RT_VISIBLE_DEVICES``， 类似如下命令"
msgstr ""

#: ../../source/advanced/npu.rst:207 c9105f687e904e11ab8717a5af8ee578
msgid "FAQ"
msgstr ""

#: ../../source/advanced/npu.rst:210 604613c6515f4dbb9c0598ec362ed329
msgid "1. 设备指定"
msgstr ""

#: ../../source/advanced/npu.rst:212 6a081d880558491684d14af5a6263780
msgid "**Q：NPU 调用失败**"
msgstr ""

#: ../../source/advanced/npu.rst:214 7fa6e349104c4560b3ab859157e35ca5
msgid "A: 通过以下两种方法排查解决："
msgstr ""

#: ../../source/advanced/npu.rst:216 183ae751207b45ce89833c96cb1f23e7
msgid ""
"通过 ``ASCEND_RT_VISIBLE_DEVICES`` 环境变量指定昇腾 NPU 卡，如 "
"``ASCEND_RT_VISIBLE_DEVICES=0,1,2,3`` 指定使用 0，1，2，3四张 NPU 卡进行微调/推理。"
msgstr ""

#: ../../source/advanced/npu.rst:220 42b6040fc16f436b9c69cd5b9bfc63f3
msgid "昇腾 NPU 卡从 0 开始编号，docker 容器内也是如此； 如映射物理机上的 6，7 号 NPU 卡到容器内使用，其对应的卡号分别为 0，1"
msgstr ""

#: ../../source/advanced/npu.rst:223 e0decee1387a424db7f2ec1fd4c48cf2
msgid ""
"检查是否安装 torch-npu，建议通过 ``pip install -e '.[torch-npu,metrics]'`` 安装 LLaMA-"
"Factory。"
msgstr ""

#: ../../source/advanced/npu.rst:227 5e14b78c630d47838e331ce6b6c4cf13
msgid "2. 推理报错"
msgstr ""

#: ../../source/advanced/npu.rst:229 8f071ad7edd942908591a5c6371d811c
msgid ""
"**Q：使用昇腾 NPU 推理报错 RuntimeError: ACL stream synchronize failed, error "
"code:507018**"
msgstr ""

#: ../../source/advanced/npu.rst:231 38bb5a900d3e44ef93cae3d8314f2ac9
msgid "A: 设置 do_sample: false，取消随机抽样策略。"
msgstr ""

#: ../../source/advanced/npu.rst:233 9205b371eee64d5fa8d47e516b391d14
msgid "比如在 yaml 中修改"
msgstr ""

#: ../../source/advanced/npu.rst:241 2b682e3c17d54ce980d65aa0f2bc26b9
msgid "比如在 api 请求中指定"
msgstr ""

#: ../../source/advanced/npu.rst:256 ../../source/advanced/npu.rst:268
#: ../../source/advanced/npu.rst:291 20d5eab36e4741f1b0a9c97df9339979
#: 36040e839386429db785b1c686677649 3714763d13ba435dbc0c1cd74c5e9507
msgid "关联 issues："
msgstr ""

#: ../../source/advanced/npu.rst:258 4ff557d70bda496eb843f675ef2d8073
msgid "https://github.com/hiyouga/LLaMA-Factory/issues/3840"
msgstr ""

#: ../../source/advanced/npu.rst:262 57fecf140c52407aa42c59d6491f99ce
msgid "3. 微调/训练报错"
msgstr ""

#: ../../source/advanced/npu.rst:264 ae1b8892af7e4f07968e3eae79ad5611
msgid ""
"**Q：使用 ChatGLM 系列模型微调/训练模型时，报错 NotImplementedError: Unknown device for "
"graph fuser**"
msgstr ""

#: ../../source/advanced/npu.rst:266 0adef6c647ea430bb25c2428683f8419
msgid ""
"A: 在 modelscope 或 huggingface 下载的 repo 里修改 ``modeling_chatglm.py`` 代码，取消 "
"torch.jit 装饰器注释"
msgstr ""

#: ../../source/advanced/npu.rst:270 3dcf4f58dd49445b920d9973731c2601
msgid "https://github.com/hiyouga/LLaMA-Factory/issues/3788"
msgstr ""

#: ../../source/advanced/npu.rst:271 a3e493a7744a4a7da5a1c8335a0a3997
msgid "https://github.com/hiyouga/LLaMA-Factory/issues/4228"
msgstr ""

#: ../../source/advanced/npu.rst:274 6f544c13932c414f8e9d908fd7a6f2d5
msgid "**Q：微调/训练启动后，HCCL 报错，包含如下关键信息：**"
msgstr ""

#: ../../source/advanced/npu.rst:289 caa60d0364a344d493bb933d9e153fc0
msgid "A: 杀掉 device 侧所有进程，等待 10s 后重新启动训练。"
msgstr ""

#: ../../source/advanced/npu.rst:293 954afd17a01744cb85d25b4b8012e2ff
msgid "https://github.com/hiyouga/LLaMA-Factory/issues/3839"
msgstr ""

#: ../../source/advanced/npu.rst:299 9c2407acaa1349edb09ab708dab146d9
msgid ""
"**Q：使用 TeleChat 模型在昇腾 NPU 推理时，报错 AssertionError： Torch not compiled with "
"CUDA enabled**"
msgstr ""

#: ../../source/advanced/npu.rst:301 419fe7ca4c72439cb746504e48f80f3f
msgid ""
"A: 此问题一般由代码中包含 cuda 相关硬编码造成，根据报错信息，找到 cuda 硬编码所在位置，对应修改为 NPU 代码。如 "
"``.cuda()`` 替换为 ``.npu()`` ； ``.to(\"cuda\")`` 替换为  ``.to(\"npu\")``"
msgstr ""

#: ../../source/advanced/npu.rst:303 0346a807f04f44499b86c0becda50c9c
msgid "**Q：模型微调遇到报错 DeviceType must be NPU. Actual DeviceType is: cpu，例如下列报错信息**"
msgstr ""

#: ../../source/advanced/npu.rst:314 43b729bf19df4d0ba682822330403abb
msgid ""
"A: 此类报错通常为部分 Tensor 未放到 NPU 上，请确保报错中算子所涉及的操作数均在 NPU "
"上。如上面的报错中，MulKernelNpuOpApi 算子为乘法算子，应确保 next_tokens 和 "
"unfinished_sequences 均已放在 NPU 上。"
msgstr ""

#: ../../source/advanced/npu.rst:317 c4702cf752834d21b7de5eef707fecf2
msgid "昇腾实践参考"
msgstr "Reference"

#: ../../source/advanced/npu.rst:319 f8edf6423fca4b67a8ad1e2bab3249fb
msgid ""
"如需更多 LLaMA-Factory × 昇腾实践指引，可参考 `全流程昇腾实践 "
"<https://ascend.github.io/docs/sources/llamafactory/example.html>`_ 。"
msgstr ""

