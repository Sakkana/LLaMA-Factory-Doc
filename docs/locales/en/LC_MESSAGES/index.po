# Translations template for PROJECT.
# Copyright (C) 2025 ORGANIZATION
# This file is distributed under the same license as the PROJECT project.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PROJECT VERSION\n"
"Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"
"POT-Creation-Date: 2025-03-05 01:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.16.0\n"

#: ../../source/index.rst:23
msgid "开始使用"
msgstr "Getting Started"

#: ../../source/index.rst:35
msgid "高级选项"
msgstr "Advanced"

#: ../../source/index.rst:2 fd8ae0c59b1749b7a93b79608f4c5bd5
msgid "Welcome to LLaMA Factory!"
msgstr "Welcome to LLaMA Factory!"

#: ../../source/index.rst:4 7e0af92b0ba8464d8c04e1509b274c52
msgid "logo"
msgstr "logo"

#: ../../source/index.rst:10 b6505192106e439997a67939acf9e331
msgid ""
"LLaMA Factory 是一个简单易用且高效的大型语言模型（Large Language Model）训练与微调平台。通过 LLaMA "
"Factory，可以在无需编写任何代码的前提下，在本地完成上百种预训练模型的微调，框架特性包括："
msgstr ""
"LLaMA Factory is an easy-to-use and efficient platform for training and fine-tuning large language models. "
"With LLaMA Factory, you can fine-tune hundreds of pre-trained models locally without writing any code. "
"Framework features include:"
#: ../../source/index.rst:12 9f1d904997c747e29bfba74eaef2ac41
msgid ""
"模型种类：LLaMA、LLaVA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi "
"等等。"
msgstr ""
"Models: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Yi, Gemma, Baichuan, ChatGLM, Phi, etc."

#: ../../source/index.rst:13 077f6949e74e422b8b48597f40d2477b
msgid "训练算法：（增量）预训练、（多模态）指令监督微调、奖励模型训练、PPO 训练、DPO 训练、KTO 训练、ORPO 训练等等。"
msgstr ""
"Trainers: (incremental) pre-training, (multimodal) instruction supervision fine-tuning, "
"reward model training, PPO training, DPO training, KTO training, ORPO training, etc."

#: ../../source/index.rst:14 9a77247f1f9840ef9c36996e4b2eb826
msgid ""
"运算精度：16 比特全参数微调、冻结微调、LoRA 微调和基于 AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ 的 "
"2/3/4/5/6/8 比特 QLoRA 微调。"
msgstr ""
"Computation Precision: 16-bit full-parameter fine-tuning, frozen fine-tuning, LoRA fine-tuning, "
"and 2/3/4/5/6/8-bit QLoRA fine-tuning based on AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ."


#: ../../source/index.rst:15 0881c8428e504cceb49f8723bee8388f
msgid ""
"优化算法：GaLore、BAdam、DoRA、LongLoRA、LLaMA Pro、Mixture-of-Depths、LoRA+、LoftQ 和"
" PiSSA。"
msgstr ""
"Optimization Algorithms: GaLore, BAdam, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ, "
"and PiSSA."

#: ../../source/index.rst:16 d7c22e3bacbe4751b4428f62016027e2
msgid "加速算子：FlashAttention-2 和 Unsloth。"
msgstr "Acceleration Operators: FlashAttention-2 and Unsloth."

#: ../../source/index.rst:17 cce1ca8fe8734f53ab760cd5c43ee245
msgid "推理引擎：Transformers 和 vLLM。"
msgstr "Inference Engines: Transformers and vLLM."

#: ../../source/index.rst:18 adc0942f4fad4149b53604d80ad6333e
msgid "实验监控：LlamaBoard、TensorBoard、Wandb、MLflow、SwanLab 等等。"
msgstr "Experiment Monitors: LlamaBoard, TensorBoard, Wandb, MLflow, SwanLab etc."

#: ../../source/index.rst:21 a08a94d2a0dc451184589260713483a3
msgid "Documentation"
msgstr "Documentation"

